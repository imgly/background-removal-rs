# Background Removal Test Dataset

This directory contains a comprehensive test dataset for validating the Rust implementation of the background removal library. The test suite ensures correctness, performance, and compatibility with the existing JavaScript implementation.

## Overview

The test dataset is designed to validate:
- **Accuracy**: Pixel-level and edge accuracy compared to ground truth
- **Performance**: Processing speed and memory usage benchmarks
- **Compatibility**: Cross-platform and JavaScript implementation compatibility
- **Format Support**: Input/output format validation

## Directory Structure

```
tests/
├── assets/
│   ├── input/
│   │   ├── portraits/          # Human subjects (10 images)
│   │   ├── products/           # E-commerce products (8 images)
│   │   ├── complex/            # Complex scenes (6 images)
│   │   ├── edge_cases/         # Challenging scenarios (8 images)
│   │   └── raw_formats/        # Raw image data samples (6 images)
│   ├── expected/
│   │   ├── masks/              # Ground truth segmentation masks
│   │   ├── javascript_output/  # Reference JavaScript results
│   │   └── benchmarks/         # Performance baseline data
│   └── metadata/
│       ├── image_specs.json    # Image metadata and properties
│       ├── test_cases.json     # Test case definitions
│       ├── validation_rules.json # Validation criteria
│       └── file_checksums.json # File integrity checksums
├── integration/
│   ├── accuracy_tests.rs       # Pixel-level accuracy validation
│   ├── performance_tests.rs    # Speed and memory benchmarks
│   ├── compatibility_tests.rs  # JavaScript output comparison
│   └── format_tests.rs         # Input/output format validation
├── tools/
│   ├── generate_reference.py   # Generate reference outputs
│   ├── validate_results.py     # Compare results
│   ├── benchmark_runner.py     # Performance testing
│   └── test_runner.sh          # Main test suite runner
└── README.md                   # This file
```

## Test Categories

### 1. Portrait Images (10 samples)
Testing human subject detection and hair edge handling:
- Single person with simple/complex backgrounds
- Multiple people in frame
- Fine hair details and edge cases
- Various lighting conditions

### 2. Product Images (8 samples)  
E-commerce product extraction validation:
- Clothing, electronics, furniture, accessories
- Different background types (white, gradient, textured)
- Transparent and reflective materials
- Shadow handling

### 3. Complex Scenes (6 samples)
Multi-subject and intricate compositions:
- Group photos and overlapping subjects
- Pets with people
- Transparent elements (glass, water)
- Mixed lighting scenarios

### 4. Edge Cases (8 samples)
Robustness and boundary condition testing:
- Extreme resolutions (64x64 to 4096x4096)
- High noise and compression artifacts
- Monochrome images
- Similar subject/background colors

### 5. Raw Formats (6 samples)
Raw image format support:
- RGB24, RGBA32, YUV420P, YUV444P
- Different color spaces (BT.601, BT.709, BT.2020)
- Various dimensions and stride configurations

## Usage

### Quick Start

Run the complete test suite:
```bash
./test_runner.sh --binary /path/to/rust/binary
```

### Individual Test Components

1. **Generate Test Data**:
```bash
cd tools/
python3 generate_reference.py --assets-dir ../assets --generate-images --generate-checksums
```

2. **Run Performance Benchmarks**:
```bash
python3 benchmark_runner.py --assets-dir ../assets --rust-binary /path/to/binary --iterations 5
```

3. **Validate Results**:
```bash
python3 validate_results.py --assets-dir ../assets --rust-output-dir /path/to/outputs
```

4. **Run Rust Integration Tests**:
```bash
cargo test --test accuracy_tests --release -- --nocapture
cargo test --test performance_tests --release -- --nocapture --ignored
```

### Configuration Options

The test runner accepts several options:
- `--binary PATH`: Path to Rust implementation binary
- `--iterations N`: Number of benchmark iterations (default: 3)
- `--timeout N`: Timeout per image in seconds (default: 30)

## Validation Criteria

### Accuracy Thresholds
- **Pixel Accuracy**: >95% for portraits, >97% for products
- **Edge Accuracy**: >90% for fine edges, >95% for clean edges
- **Structural Similarity**: >85% overall compatibility

### Performance Targets
- **Processing Time**: 2-5x improvement over JavaScript baseline
- **Memory Usage**: Efficient scaling with image complexity
- **Throughput**: Competitive batch processing performance

### Format Requirements
- **Input Formats**: JPEG, PNG, WebP, RGB24, RGBA32, YUV420P, YUV444P
- **Output Formats**: PNG (alpha), JPEG (background), WebP (alpha), raw formats
- **Color Spaces**: sRGB, BT.601, BT.709, BT.2020

## Test Data Generation

The test dataset uses synthetic images generated by `generate_reference.py`. These images are designed to:
- Cover a wide range of scenarios and edge cases
- Provide consistent and reproducible test conditions
- Enable automated validation without copyright concerns

For production validation, the synthetic images should be supplemented with real-world test data.

## Interpreting Results

### Accuracy Reports
- **Pixel Accuracy**: Percentage of correctly classified pixels within tolerance
- **Edge Accuracy**: Accuracy at detected edge pixels (more stringent)
- **SSIM**: Structural similarity index (0-1, higher is better)

### Performance Reports
- **Processing Time**: Wall-clock time including I/O
- **Memory Usage**: Peak memory consumption during processing
- **Throughput**: Images processed per second

### Compatibility Reports
- **JavaScript Compatibility**: Pixel-level comparison with reference implementation
- **Cross-Platform**: Deterministic output across target platforms
- **Format Support**: Successful processing of all input/output format combinations

## Continuous Integration

The test suite is designed for automated CI/CD integration:

```yaml
# .github/workflows/test.yml example
- name: Run Test Suite
  run: |
    cargo build --release
    ./tests/test_runner.sh --binary target/release/bg-remove-standard
    
- name: Upload Test Results
  uses: actions/upload-artifact@v3
  with:
    name: test-results
    path: test_results/
```

## Extending the Test Suite

### Adding New Test Cases

1. Add image specifications to `assets/metadata/image_specs.json`
2. Update test case definitions in `assets/metadata/test_cases.json`
3. Place test images in appropriate category directories
4. Regenerate checksums: `python3 generate_reference.py --generate-checksums`

### Custom Validation Rules

Modify `assets/metadata/validation_rules.json` to adjust:
- Accuracy thresholds
- Performance requirements
- Format specifications
- Compatibility criteria

### Platform-Specific Testing

The framework supports testing across multiple platforms:
- macOS (x64, ARM64)
- Linux (x64, ARM64)
- Future: Windows, iOS, Android

## Dependencies

### Python Requirements
```bash
pip3 install pillow numpy psutil scikit-image serde-json
```

### Rust Requirements
- Rust 1.70+ with 2021 edition
- `image`, `serde_json` crates for integration tests

## License

This test dataset is provided under the same license as the main project (AGPL v3). Synthetic test images are generated and do not contain copyrighted material.

## Contributing

When contributing to the test suite:
1. Ensure new tests cover meaningful scenarios
2. Follow existing naming conventions
3. Update documentation for new test categories
4. Verify tests pass on all target platforms

For questions or issues with the test suite, please open an issue in the main project repository.